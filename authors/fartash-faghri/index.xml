<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fartash Faghri | Qingru Zhang</title>
    <link>https://qingruzhang.github.io/authors/fartash-faghri/</link>
      <atom:link href="https://qingruzhang.github.io/authors/fartash-faghri/index.xml" rel="self" type="application/rss+xml" />
    <description>Fartash Faghri</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 25 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://qingruzhang.github.io/img/icon-192.png</url>
      <title>Fartash Faghri</title>
      <link>https://qingruzhang.github.io/authors/fartash-faghri/</link>
    </image>
    
    <item>
      <title>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</title>
      <link>https://qingruzhang.github.io/project/svrg_project/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/project/svrg_project/</guid>
      <description>

&lt;hr /&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence. Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence. Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs.&lt;/p&gt;

&lt;h3 id=&#34;our-contribution&#34;&gt;Our Contribution&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We show the exact expected loss of SVRG and SGD as a function of iterations and computational cost.&lt;/li&gt;
&lt;li&gt;We discuss the trade-offs between the total computational cost and convergence performance.&lt;/li&gt;
&lt;li&gt;We consider two different training regimes with and without label noise.

&lt;ul&gt;
&lt;li&gt;Under noisy labels, the analysis suggests SGD only outperforms SVRG under a mild total computational cost.&lt;/li&gt;
&lt;li&gt;However, SGD always exhibits a faster convergence compared to SVRG when there is no label noise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Numerical experiments validate our theoretical predictions on both MNIST and CIFAR-10.

&lt;ul&gt;
&lt;li&gt;In particular, the comparison on underparameterized neural networks closely matches with our noisy least squares model prediction.&lt;/li&gt;
&lt;li&gt;Whereas, the effect of overparameterization is captured by the regression model without label noise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;svrg-inner-outer-loop-algorithm&#34;&gt;SVRG:  inner-outer loop algorithm.&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;In the outer loop:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every $T$ steps, we evaluate a large batch gradient $\overline{\mathbf{g}}=\frac{1}{N} \sum_{i}^{N} \nabla_{\boldsymbol{\theta}^{( m T )}} L_{i}$. where $N \gg b$, and $m$ is the outer loop index.&lt;/li&gt;
&lt;li&gt;We store the parameters at reference points $\boldsymbol{\theta}^{(m T)}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;In the inner loop:&lt;/strong&gt;
$$\boldsymbol{\theta}^{(m T+t+1)}=\boldsymbol{\theta}^{(m T+t)}-\alpha^{(t)}\left(\hat{\boldsymbol{g}}^{(m T+t)}-\tilde{\boldsymbol{g}}^{(m T+t)}+\overline{\mathbf{g}}\right)$$&lt;/p&gt;

&lt;p&gt;where $ \hat{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T+t)}} L_{i} $ is the current batch gradient and $ \tilde{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T)}} L_{i}$ is the old gradient.&lt;/p&gt;

&lt;h3 id=&#34;our-model-the-noisy-least-squares-regression-model&#34;&gt;Our Model: The noisy least squares regression model&lt;/h3&gt;

&lt;p&gt;&lt;a name=&#34;model&#34;&gt;&lt;/a&gt;
The input data is d-dimensional, and the output label is generated by a
linear teacher model with additive noise:&lt;/p&gt;

&lt;p&gt;$$
\left(\boldsymbol{x}_{i}, \epsilon_{i}\right) \sim P_{x} \times P_{\epsilon} ; \quad y_{i}=\boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{*}+\epsilon_{i}
$$
where $\mathbb{E}\left[\boldsymbol{x}_{i}\right]=\boldsymbol{\mu} \in \mathbb{R}^{d} \text { and } \operatorname{Cov}\left(\boldsymbol{x}_{i}\right)=\Sigma, \mathbb{E}\left[\epsilon_{i}\right]=0, \operatorname{Var}\left(\epsilon_{i}\right)=\sigma_{y}^{2}$.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;assumptions&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;assumptions&#34;&gt;Assumptions:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;$\mu=\mathbf{0}$.&lt;/li&gt;
&lt;li&gt;$\Sigma$ is diagonal.&lt;/li&gt;
&lt;li&gt;$\boldsymbol{\theta}^{*}=\mathbf{0}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;loss-function-and-bias-variance-decomposition&#34;&gt;Loss Function and Bias-Variance Decomposition:&lt;/h4&gt;

&lt;p&gt;Under our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, the &lt;strong&gt;expected loss&lt;/strong&gt; can be simplified as a function of the second moment of the iterate.&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
L\left(\boldsymbol{\theta^{(t)}}\right)=&amp;amp; \frac{1}{2} \mathbb{E}\left[\left(\boldsymbol{x_{i}}^{\top} \boldsymbol{\theta^{(t)}}-\epsilon_{i}\right)^{2}\right]\\&lt;br /&gt;
=&amp;amp; \frac{1}{2}\left(\operatorname{tr}\left(\Sigma \mathbb{E}\left[\boldsymbol{\theta^{(t)}} \boldsymbol{ \theta^{(t)}}\right]\right)+\sigma_{y}^{2}\right)\\&lt;br /&gt;
=&amp;amp; \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}
\end{aligned}
$$&lt;/p&gt;

&lt;h4 id=&#34;mini-batch-gradients&#34;&gt;Mini-batch gradients:&lt;/h4&gt;

&lt;p&gt;$$
\hat{\boldsymbol{g}}^{(t)}=\frac{1}{b} \sum_{i}^{b}\left(\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{(t)}-\boldsymbol{x}_{i} \epsilon_{i}\right)=X_{b} X_{b}^{\top} \boldsymbol{\theta}^{(t)}-\frac{1}{\sqrt{b}} X_{b} \boldsymbol{\epsilon}_{b}
$$&lt;/p&gt;

&lt;p&gt;where $X_{b}=\frac{1}{\sqrt{b}}\left[\boldsymbol{x}_{1} ; \boldsymbol{x}_{2} ; \cdots ; \boldsymbol{x}_{b}\right] \in \mathbb{R}^{d \times b}$ and the target noise vector $\boldsymbol{\epsilon}_{b}=\left[\epsilon_{1} ; \epsilon_{2} ; \cdots ; \epsilon_{b}\right]^{\top} \in \mathbb{R}^{b}$.&lt;/p&gt;

&lt;h3 id=&#34;notations-and-definitions&#34;&gt;Notations and Definitions&lt;/h3&gt;

&lt;p&gt;$$
\begin{array}{l}{\mathrm{M}(\theta)=\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right], \quad \mathbf{m}(\theta)=\operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right]\right)} \\&lt;br /&gt;
{\mathrm{C}(\boldsymbol{\theta})=\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top} \boldsymbol{\theta} \boldsymbol{\theta}^{\top} \boldsymbol{x} \boldsymbol{x}^{\top}\right]-\Sigma \mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right] \Sigma} \\&lt;br /&gt;
{V=\alpha^{2} \sigma_{y}^{2} \operatorname{diag}(\Sigma) \\&lt;br /&gt;
R=(\mathrm{I}-\alpha \Sigma)^{2}+\frac{\alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)} \\&lt;br /&gt;
{Q=\frac{2 \alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right), \quad P=\mathrm{I}-\alpha \Sigma} \\&lt;br /&gt;
{F=\frac{2 \alpha^{2}(N+b)}{N b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)}\end{array}
$$&lt;/p&gt;

&lt;h3 id=&#34;the-dynamic-of-sgd&#34;&gt;The Dynamic of SGD&lt;/h3&gt;

&lt;p&gt;$$
\mathrm{M}\Big(\boldsymbol{\theta}^{(t+1)}\Big)=\underbrace{(\mathrm{I}-\alpha \Sigma) \mathrm{M}\Big(\boldsymbol{\theta}^{(t)}\Big)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}
+ \underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big( \mathrm{M}(\boldsymbol{\theta}^{(t)})\Big)}_{  {2}:\text { input noise } }
+\underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{b} \Sigma}_{{3}: \text { label noise }}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The term $1$ leads to an exponential shrinkage of the loss due to the gradient descent update.&lt;/li&gt;
&lt;li&gt;Since we are using a noisy gradient, the second term $2$ represents the variance of stochastic gradient caused by the random input $ X_b $.&lt;/li&gt;
&lt;li&gt;The term $3$ comes from the label noise $ \epsilon_b $.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;the-expected-second-moment-for-sgd&#34;&gt;The expected second moment for SGD&lt;/h4&gt;

&lt;p&gt;Given the &lt;a href=&#34;#model&#34;&gt;model&lt;/a&gt; and our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, we have the second moment of parameter as:&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}\Big(\boldsymbol{\theta}^{(t)}\Big)=R^{t}\left(\mathbf{m}\left(\boldsymbol{\theta}^{(0)}\right)-\frac{(\mathrm{I}-R)^{-1} V}{b}\right)+\frac{(\mathrm{I}-R)^{-1} V}{b}
$$&lt;/p&gt;

&lt;p&gt;Since $L\left(\boldsymbol{\theta^{(t)}}\right)= \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}$, we can draw the exact expression of the expected loss.&lt;/p&gt;

&lt;h3 id=&#34;the-dilemma-for-svrg&#34;&gt;The Dilemma for SVRG&lt;/h3&gt;

&lt;h4 id=&#34;svrg-dynamic-under-our-model&#34;&gt;SVRG Dynamic under Our Model&lt;/h4&gt;

&lt;p&gt;$$
\begin{aligned} \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t+1)}\right)=&amp;amp;\underbrace{(I-\alpha \Sigma) \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t)}\right)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}+\underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T+t)})\Big)}_{ 2: \text { input noise } } \\&lt;br /&gt;
&amp;amp; + \underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{N} \Sigma}_{ 3: \text { label noise }} + \underbrace{\alpha^{2} \frac{N+b}{N b} \mathrm{C}\left(\mathrm{M}\left(\boldsymbol{\theta}^{(m T)}\right)\right)}_{ 4: \text {Gvariance due to } \tilde{\mathbf{g}}^{(m T+t)}}\\&lt;br /&gt;
&amp;amp; \underbrace{-\frac{\alpha^{2}}{b}\left(\mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T)}) P^{t}\Big) +\mathrm{C}\Big(P^{t} \mathrm{M}(\boldsymbol{\theta}^{(m T)})\Big)\right).}_{\mathbb{5} \text { Variance reduction from control variate }} \end{aligned}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First notice that terms ${1}, {2}, {3}$ reappear, contributed by the SGD update.&lt;/li&gt;
&lt;li&gt;The additional terms, ${4}$ and ${5}$, are due to the control variate.&lt;/li&gt;
&lt;li&gt;Observe that the variance reduction term ${5}$ decays exponentially throughout the inner loop, with decay rate $I-\alpha\Sigma$, which is the same term that governs the decay rate of the term ${1}$, hence resulting in a conflict between the two.&lt;/li&gt;
&lt;li&gt;If we want to reduce the term ${1}$ as fast as possible, we would prefer a large learning rate, i.e. $\alpha \to \frac{1}{\lambda_{\max}(\Sigma)}$. But this will also make the boosts provided by the control variate diminish rapidly, leading to a poor variance reduction.&lt;/li&gt;
&lt;li&gt;The term ${4}$ makes things even worse as it will maintain as a constant throughout the inner loop, contributing to an extra variance on top of the variance from standard SGD.&lt;/li&gt;
&lt;li&gt;On the other hand, if one chooses a small learning rate for the variance reduction to take effect, this inevitably will make the decay rate for term ${1}$ smaller, resulting in a slower convergence.&lt;/li&gt;
&lt;li&gt;A good news for SVRG is that the label noise (term ${3}$) is scaled by $\frac{b}{N}$, which lets SVRG converge to a lower loss value than SGD &amp;ndash; a strict advantage of SVRG compared to SGD.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;the-expected-second-moment-for-svrg&#34;&gt;The expected second moment for SVRG&lt;/h4&gt;

&lt;p&gt;Given the &lt;a href=&#34;#model&#34;&gt;model&lt;/a&gt; and our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, we have the second moment of parameter as:&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}\left(\boldsymbol{\theta}^{((m+1) T)}\right)=\lambda(\alpha, b, T, N, \Sigma) \mathbf{m}\left(\boldsymbol{\theta}^{(m T)}\right)+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} \frac{V}{N}
$$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$
\lambda(\alpha, b, T, N, \Sigma)=R^{T}-\left(\sum_{k=0}^{T-1} R^{k} Q P^{-k}\right) P^{T-1}+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} F
$$&lt;/p&gt;

&lt;h3 id=&#34;the-numerical-experiments-based-on-our-dynamic&#34;&gt;The Numerical Experiments Based on Our Dynamic&lt;/h3&gt;

&lt;p&gt;In all of our experiments, we compare SVRG and SGD under a fixed computation budget. For every budget, we run 10 leanring rates expentionally varying from 0.1 to 0.001, typically, for SGD. For SVRG, besides turning learning rate, we also choose different pairs of batch size and snapshot interval under a fixed budget. Then, at each step, we plot the minimum loss over these setting. In short, every line in the following plots need to run 10 or more setting for SGD; and 30 or more for SVRG. And the x-axis &amp;ldquo;epoch&amp;rdquo; denotes the total computational cost normalized by dataset.&lt;/p&gt;

&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;!-- &lt;img src=&#34;../../img/AdaShift/warm-up1.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt; --&gt;
    &lt;div style=&#34;width: 350px; font-size:80%; text-align:center;&#34;&gt;&lt;img src=&#34;../../img/SVRG_Project/sim1.png&#34; alt=&#34;alternate text 1&#34; width=&#34;width&#34; height=&#34;height&#34; style=&#34;padding-bottom:0.5em;&#34; /&gt; With Label Noise &lt;/div&gt;
  &lt;/div&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
  &lt;div class=&#34;column&#34;&gt;
    &lt;div style=&#34;width:350px; font-size:80%; text-align:center;&#34;&gt;&lt;img src=&#34;../../img/SVRG_Project/sim2.png&#34; alt=&#34;alternate text2&#34; width=&#34;width&#34; height=&#34;height&#34; style=&#34;padding-bottom:0.5em;&#34; /&gt;Without Label Noise&lt;/div&gt;
    &lt;!-- &lt;img src=&#34;../../img/AdaShift/warm-up2.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt; --&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Our Conclusions:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The case with label noise&lt;/strong&gt;: The plot demonstrated an explicit trade-off between computational cost and convergence speed.

&lt;ul&gt;
&lt;li&gt;a crossing point of between SGD and SVRG appear, indicating SGD achieved a faster convergence speed in the first phase of the training, but converged to a higher loss, for all per-iteration compute cost.&lt;/li&gt;
&lt;li&gt;The per-iteration computational cost does not seem to affect the time crossing point takes place. For all these three costs, the crossing points in the plot are at around the same time: $5.5$ epochs.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The case of no label noise&lt;/strong&gt;: Both methods achieved linear convergence, while SGD achieved a much faster rate than SVRG, showing absolute dominance in this regime.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;experiments-on-benchmark-datasets&#34;&gt;Experiments on Benchmark Datasets&lt;/h3&gt;

&lt;h4 id=&#34;underparameterized-setting&#34;&gt;Underparameterized Setting&lt;/h4&gt;

&lt;p&gt;The results in underparametrized regime corresponds to the analysis with label noise. $ \mathbb{E}&lt;em&gt;{ \boldsymbol{x&lt;/em&gt;{i}} \in P&lt;em&gt;{\boldsymbol{x}}  } \left[ \left(\boldsymbol{x&lt;/em&gt;{i}}^{\top} \boldsymbol{\theta^{(t)}} - \epsilon_{i} \right)^{2} \right] = \mathbb{E}&lt;em&gt;{ \ boldsymbol{X} \in P&lt;/em&gt;{ \boldsymbol{X} } } \left[ \frac{1}{N} \sum&lt;em&gt;{ \boldsymbol{x&lt;/em&gt;{i}} \in \boldsymbol{X} } \left(\boldsymbol{x_{i}}^{\top} \boldsymbol{\theta^{(t)}} - \epsilon_{i} \right)^{2} \right]  $&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Logistic Regression on MNIST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../img/SVRG_Project/logreg.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MLP-10-10 on MNIST&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/mlp10.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Underparametrized CNN on CIFAR-10&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/sscnn.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;h4 id=&#34;overparametrized-setting&#34;&gt;Overparametrized Setting&lt;/h4&gt;

&lt;p&gt;The results in overparametrized regime corresponds to the analysis without label noise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overparametrized MLP on MNIST&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/mlp1024.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overparametrized CNN on CIFAR-10&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/cnn.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</title>
      <link>https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/</guid>
      <description>&lt;p&gt;Further details.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
