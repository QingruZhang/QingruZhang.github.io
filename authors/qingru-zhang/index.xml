<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>**Qingru Zhang** | Qingru Zhang</title>
    <link>https://qingruzhang.github.io/authors/qingru-zhang/</link>
      <atom:link href="https://qingruzhang.github.io/authors/qingru-zhang/index.xml" rel="self" type="application/rss+xml" />
    <description>**Qingru Zhang**</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 25 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://qingruzhang.github.io/img/icon-192.png</url>
      <title>**Qingru Zhang**</title>
      <link>https://qingruzhang.github.io/authors/qingru-zhang/</link>
    </image>
    
    <item>
      <title>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</title>
      <link>https://qingruzhang.github.io/project/svrg_project/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/project/svrg_project/</guid>
      <description>

&lt;hr /&gt;

&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;

&lt;p&gt;SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence. Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence. Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs.&lt;/p&gt;

&lt;h3 id=&#34;our-contribution&#34;&gt;Our Contribution&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;We show the exact expected loss of SVRG and SGD as a function of iterations and computational cost.&lt;/li&gt;
&lt;li&gt;We discuss the trade-offs between the total computational cost and convergence performance.&lt;/li&gt;
&lt;li&gt;We consider two different training regimes with and without label noise.

&lt;ul&gt;
&lt;li&gt;Under noisy labels, the analysis suggests SGD only outperforms SVRG under a mild total computational cost.&lt;/li&gt;
&lt;li&gt;However, SGD always exhibits a faster convergence compared to SVRG when there is no label noise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Numerical experiments validate our theoretical predictions on both MNIST and CIFAR-10.

&lt;ul&gt;
&lt;li&gt;In particular, the comparison on underparameterized neural networks closely matches with our noisy least squares model prediction.&lt;/li&gt;
&lt;li&gt;Whereas, the effect of overparameterization is captured by the regression model without label noise.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;svrg-inner-outer-loop-algorithm&#34;&gt;SVRG:  inner-outer loop algorithm.&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;In the outer loop:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For every $T$ steps, we evaluate a large batch gradient $\overline{\mathbf{g}}=\frac{1}{N} \sum_{i}^{N} \nabla_{\boldsymbol{\theta}^{( m T )}} L_{i}$. where $N \gg b$, and $m$ is the outer loop index.&lt;/li&gt;
&lt;li&gt;We store the parameters at reference points $\boldsymbol{\theta}^{(m T)}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;In the inner loop:&lt;/strong&gt;
$$\boldsymbol{\theta}^{(m T+t+1)}=\boldsymbol{\theta}^{(m T+t)}-\alpha^{(t)}\left(\hat{\boldsymbol{g}}^{(m T+t)}-\tilde{\boldsymbol{g}}^{(m T+t)}+\overline{\mathbf{g}}\right)$$&lt;/p&gt;

&lt;p&gt;where $ \hat{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T+t)}} L_{i} $ is the current batch gradient and $ \tilde{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T)}} L_{i}$ is the old gradient.&lt;/p&gt;

&lt;h3 id=&#34;our-model-the-noisy-least-squares-regression-model&#34;&gt;Our Model: The noisy least squares regression model&lt;/h3&gt;

&lt;p&gt;&lt;a name=&#34;model&#34;&gt;&lt;/a&gt;
The input data is d-dimensional, and the output label is generated by a
linear teacher model with additive noise:&lt;/p&gt;

&lt;p&gt;$$
\left(\boldsymbol{x}_{i}, \epsilon_{i}\right) \sim P_{x} \times P_{\epsilon} ; \quad y_{i}=\boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{*}+\epsilon_{i}
$$
where $\mathbb{E}\left[\boldsymbol{x}_{i}\right]=\boldsymbol{\mu} \in \mathbb{R}^{d} \text { and } \operatorname{Cov}\left(\boldsymbol{x}_{i}\right)=\Sigma, \mathbb{E}\left[\epsilon_{i}\right]=0, \operatorname{Var}\left(\epsilon_{i}\right)=\sigma_{y}^{2}$.&lt;/p&gt;

&lt;p&gt;&lt;a name=&#34;assumptions&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&#34;assumptions&#34;&gt;Assumptions:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;$\mu=\mathbf{0}$.&lt;/li&gt;
&lt;li&gt;$\Sigma$ is diagonal.&lt;/li&gt;
&lt;li&gt;$\boldsymbol{\theta}^{*}=\mathbf{0}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;loss-function-and-bias-variance-decomposition&#34;&gt;Loss Function and Bias-Variance Decomposition:&lt;/h4&gt;

&lt;p&gt;Under our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, the &lt;strong&gt;expected loss&lt;/strong&gt; can be simplified as a function of the second moment of the iterate.&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
L\left(\boldsymbol{\theta^{(t)}}\right)=&amp;amp; \frac{1}{2} \mathbb{E}\left[\left(\boldsymbol{x_{i}}^{\top} \boldsymbol{\theta^{(t)}}-\epsilon_{i}\right)^{2}\right]\\&lt;br /&gt;
=&amp;amp; \frac{1}{2}\left(\operatorname{tr}\left(\Sigma \mathbb{E}\left[\boldsymbol{\theta^{(t)}} \boldsymbol{ \theta^{(t)}}\right]\right)+\sigma_{y}^{2}\right)\\&lt;br /&gt;
=&amp;amp; \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}
\end{aligned}
$$&lt;/p&gt;

&lt;h4 id=&#34;mini-batch-gradients&#34;&gt;Mini-batch gradients:&lt;/h4&gt;

&lt;p&gt;$$
\hat{\boldsymbol{g}}^{(t)}=\frac{1}{b} \sum_{i}^{b}\left(\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{(t)}-\boldsymbol{x}_{i} \epsilon_{i}\right)=X_{b} X_{b}^{\top} \boldsymbol{\theta}^{(t)}-\frac{1}{\sqrt{b}} X_{b} \boldsymbol{\epsilon}_{b}
$$&lt;/p&gt;

&lt;p&gt;where $X_{b}=\frac{1}{\sqrt{b}}\left[\boldsymbol{x}_{1} ; \boldsymbol{x}_{2} ; \cdots ; \boldsymbol{x}_{b}\right] \in \mathbb{R}^{d \times b}$ and the target noise vector $\boldsymbol{\epsilon}_{b}=\left[\epsilon_{1} ; \epsilon_{2} ; \cdots ; \epsilon_{b}\right]^{\top} \in \mathbb{R}^{b}$.&lt;/p&gt;

&lt;h3 id=&#34;notations-and-definitions&#34;&gt;Notations and Definitions&lt;/h3&gt;

&lt;p&gt;$$
\begin{array}{l}{\mathrm{M}(\theta)=\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right], \quad \mathbf{m}(\theta)=\operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right]\right)} \\&lt;br /&gt;
{\mathrm{C}(\boldsymbol{\theta})=\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top} \boldsymbol{\theta} \boldsymbol{\theta}^{\top} \boldsymbol{x} \boldsymbol{x}^{\top}\right]-\Sigma \mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right] \Sigma} \\&lt;br /&gt;
{V=\alpha^{2} \sigma_{y}^{2} \operatorname{diag}(\Sigma) \\&lt;br /&gt;
R=(\mathrm{I}-\alpha \Sigma)^{2}+\frac{\alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)} \\&lt;br /&gt;
{Q=\frac{2 \alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right), \quad P=\mathrm{I}-\alpha \Sigma} \\&lt;br /&gt;
{F=\frac{2 \alpha^{2}(N+b)}{N b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)}\end{array}
$$&lt;/p&gt;

&lt;h3 id=&#34;the-dynamic-of-sgd&#34;&gt;The Dynamic of SGD&lt;/h3&gt;

&lt;p&gt;$$
\mathrm{M}\left(\boldsymbol{\theta}^{(t+1)}\right)=\underbrace{(\mathrm{I}-\alpha \Sigma) \mathrm{M}\left(\boldsymbol{\theta}^{(t)}\right)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}
+ \underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big( \mathrm{M}(\boldsymbol{\theta}^{(t)})\Big)}_{  {2}:\text { input noise } }
+\underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{b} \Sigma}_{{3}: \text { label noise }}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The term $1$ leads to an exponential shrinkage of the loss due to the gradient descent update.&lt;/li&gt;
&lt;li&gt;Since we are using a noisy gradient, the second term $2$ represents the variance of stochastic gradient caused by the random input $ X_b $.&lt;/li&gt;
&lt;li&gt;The term $3$ comes from the label noise $ \epsilon_b $.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;the-expected-second-moment-for-sgd&#34;&gt;The expected second moment for SGD&lt;/h4&gt;

&lt;p&gt;Given the &lt;a href=&#34;#model&#34;&gt;model&lt;/a&gt; and our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, we have the second moment of parameter as:&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}\left(\boldsymbol{\theta}^{(t)}\right)=R^{t}\left(\mathbf{m}\left(\boldsymbol{\theta}^{(0)}\right)-\frac{(\mathrm{I}-R)^{-1} V}{b}\right)+\frac{(\mathrm{I}-R)^{-1} V}{b}
$$&lt;/p&gt;

&lt;p&gt;Since $L\left(\boldsymbol{\theta^{(t)}}\right)= \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}$, we can draw the exact expression of the expected loss.&lt;/p&gt;

&lt;h3 id=&#34;the-dilemma-for-svrg&#34;&gt;The Dilemma for SVRG&lt;/h3&gt;

&lt;h4 id=&#34;svrg-dynamic-under-our-model&#34;&gt;SVRG Dynamic under Our Model&lt;/h4&gt;

&lt;p&gt;$$
\begin{aligned} \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t+1)}\right)=&amp;amp;\underbrace{(I-\alpha \Sigma) \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t)}\right)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}+\underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T+t)})\Big)}_{ 2: \text { input noise } } \\&lt;br /&gt;
&amp;amp; + \underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{N} \Sigma}_{ 3: \text { label noise }} + \underbrace{\alpha^{2} \frac{N+b}{N b} \mathrm{C}\left(\mathrm{M}\left(\boldsymbol{\theta}^{(m T)}\right)\right)}_{ 4: \text {Gvariance due to } \tilde{\mathbf{g}}^{(m T+t)}}\\&lt;br /&gt;
&amp;amp; \underbrace{-\frac{\alpha^{2}}{b}\left(\mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T)}) P^{t}\Big) +\mathrm{C}\Big(P^{t} \mathrm{M}(\boldsymbol{\theta}^{(m T)})\Big)\right).}_{\mathbb{5} \text { Variance reduction from control variate }} \end{aligned}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First notice that terms ${1}, {2}, {3}$ reappear, contributed by the SGD update.&lt;/li&gt;
&lt;li&gt;The additional terms, ${4}$ and ${5}$, are due to the control variate.&lt;/li&gt;
&lt;li&gt;Observe that the variance reduction term ${5}$ decays exponentially throughout the inner loop, with decay rate $I-\alpha\Sigma$, which is the same term that governs the decay rate of the term ${1}$, hence resulting in a conflict between the two.&lt;/li&gt;
&lt;li&gt;If we want to reduce the term ${1}$ as fast as possible, we would prefer a large learning rate, i.e. $\alpha \to \frac{1}{\lambda_{\max}(\Sigma)}$. But this will also make the boosts provided by the control variate diminish rapidly, leading to a poor variance reduction.&lt;/li&gt;
&lt;li&gt;The term ${4}$ makes things even worse as it will maintain as a constant throughout the inner loop, contributing to an extra variance on top of the variance from standard SGD.&lt;/li&gt;
&lt;li&gt;On the other hand, if one chooses a small learning rate for the variance reduction to take effect, this inevitably will make the decay rate for term ${1}$ smaller, resulting in a slower convergence.&lt;/li&gt;
&lt;li&gt;A good news for SVRG is that the label noise (term ${3}$) is scaled by $\frac{b}{N}$, which lets SVRG converge to a lower loss value than SGD &amp;ndash; a strict advantage of SVRG compared to SGD.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;the-expected-second-moment-for-svrg&#34;&gt;The expected second moment for SVRG&lt;/h4&gt;

&lt;p&gt;Given the &lt;a href=&#34;#model&#34;&gt;model&lt;/a&gt; and our &lt;a href=&#34;#assumptions&#34;&gt;assumptions&lt;/a&gt;, we have the second moment of parameter as:&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}\left(\boldsymbol{\theta}^{((m+1) T)}\right)=\lambda(\alpha, b, T, N, \Sigma) \mathbf{m}\left(\boldsymbol{\theta}^{(m T)}\right)+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} \frac{V}{N}
$$&lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$
\lambda(\alpha, b, T, N, \Sigma)=R^{T}-\left(\sum_{k=0}^{T-1} R^{k} Q P^{-k}\right) P^{T-1}+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} F
$$&lt;/p&gt;

&lt;h3 id=&#34;the-numerical-experiments-based-on-our-dynamic&#34;&gt;The Numerical Experiments Based on Our Dynamic&lt;/h3&gt;

&lt;p&gt;In all of our experiments, we compare SVRG and SGD under a fixed computation budget. For every budget, we run 10 leanring rates expentionally varying from 0.1 to 0.001, typically, for SGD. For SVRG, besides turning learning rate, we also choose different pairs of batch size and snapshot interval under a fixed budget. Then, at each step, we plot the minimum loss over these setting. In short, every line in the following plots need to run 10 or more setting for SGD; and 30 or more for SVRG. And the x-axis &amp;ldquo;epoch&amp;rdquo; denotes the total computational cost normalized by dataset.&lt;/p&gt;

&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;!-- &lt;img src=&#34;../../img/AdaShift/warm-up1.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt; --&gt;
    &lt;div style=&#34;width: 350px; font-size:80%; text-align:center;&#34;&gt;&lt;img src=&#34;../../img/SVRG_Project/sim1.png&#34; alt=&#34;alternate text 1&#34; width=&#34;width&#34; height=&#34;height&#34; style=&#34;padding-bottom:0.5em;&#34; /&gt; With Label Noise &lt;/div&gt;
  &lt;/div&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
  &lt;div class=&#34;column&#34;&gt;
    &lt;div style=&#34;width:350px; font-size:80%; text-align:center;&#34;&gt;&lt;img src=&#34;../../img/SVRG_Project/sim2.png&#34; alt=&#34;alternate text2&#34; width=&#34;width&#34; height=&#34;height&#34; style=&#34;padding-bottom:0.5em;&#34; /&gt;Without Label Noise&lt;/div&gt;
    &lt;!-- &lt;img src=&#34;../../img/AdaShift/warm-up2.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt; --&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Our Conclusions:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The case with label noise&lt;/strong&gt;: The plot demonstrated an explicit trade-off between computational cost and convergence speed.

&lt;ul&gt;
&lt;li&gt;a crossing point of between SGD and SVRG appear, indicating SGD achieved a faster convergence speed in the first phase of the training, but converged to a higher loss, for all per-iteration compute cost.&lt;/li&gt;
&lt;li&gt;The per-iteration computational cost does not seem to affect the time crossing point takes place. For all these three costs, the crossing points in the plot are at around the same time: $5.5$ epochs.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The case of no label noise&lt;/strong&gt;: Both methods achieved linear convergence, while SGD achieved a much faster rate than SVRG, showing absolute dominance in this regime.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;experiments-on-benchmark-datasets&#34;&gt;Experiments on Benchmark Datasets&lt;/h3&gt;

&lt;h4 id=&#34;underparameterized-setting&#34;&gt;Underparameterized Setting&lt;/h4&gt;

&lt;p&gt;The results in underparametrized regime corresponds to the analysis with label noise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Logistic Regression on MNIST&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../../img/SVRG_Project/logreg.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MLP-10-10 on MNIST&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/mlp10.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Underparametrized CNN on CIFAR-10&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/sscnn.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;h4 id=&#34;overparametrized-setting&#34;&gt;Overparametrized Setting&lt;/h4&gt;

&lt;p&gt;The results in overparametrized regime corresponds to the analysis without label noise.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overparametrized MLP on MNIST&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/mlp1024.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overparametrized CNN on CIFAR-10&lt;/strong&gt;
&lt;img src=&#34;../../img/SVRG_Project/cnn.png&#34; alt=&#34;drawing&#34; style=&#34;width:450px; &#34;/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</title>
      <link>https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/</guid>
      <description>&lt;p&gt;Further details.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Survey on the generalization theory of neural network</title>
      <link>https://qingruzhang.github.io/project/generalization_survey_project/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/project/generalization_survey_project/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Conducted a survey on the generalization theory and its related work.&lt;/li&gt;
&lt;li&gt;Studied the proof of various generalization bounds and obtained a deep understanding on PAC-Bayesian framework, Rademacher complexity and VC-dimension.&lt;/li&gt;
&lt;li&gt;Considered to explain the generalization of neural network from dataset structure and&lt;/li&gt;
&lt;li&gt;Presented a group talk about generalization bounds and rethinking generalization.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods</title>
      <link>https://qingruzhang.github.io/project/adashiftproject/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/project/adashiftproject/</guid>
      <description>

&lt;hr /&gt;

&lt;h3 id=&#34;the-nonconvergence-of-adam&#34;&gt;The Nonconvergence of Adam&lt;/h3&gt;

&lt;p&gt;Update rule of Adam:
$$
\begin{aligned}
&amp;amp; g_t = \nabla f_t\\&lt;br /&gt;
&amp;amp; m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t\label{eq:1}\\&lt;br /&gt;
&amp;amp; v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2\label{ew:2}\\\&lt;br /&gt;
&amp;amp;\theta_{t+1} = \theta_t - \frac{\alpha_t}{\sqrt{v_t}}m_t
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://openreview.net/forum?id=ryQu7f-RZ&#34; target=&#34;_blank&#34;&gt;Reddi et al.&lt;/a&gt; pointed out two type of non-convergence problem for Adam:
&lt;a name=&#34;countereg&#34;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sequential Counterexample:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$f_{t}(\theta)=\left\{\begin{array}{ll}{C \theta,} &amp;amp; {\text { if } t \bmod d=1} \\ {-\theta,} &amp;amp; {\text { otherwise }}\end{array}\right.$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Stochastic Counterexample:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
f_{t}(\theta)=\left\{\begin{array}{ll}{C \theta,} &amp;amp; {\text { with probability } p=\frac{1+\delta}{C+1}} \\ {-\theta,} &amp;amp; {\text { with probability } 1-p=\frac{C-\delta}{C+1}}\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;and they claim after fixing the issue about positive definiteness of $\Gamma_{t+1}=\frac{\sqrt{V_{t+1}}}{\alpha_{t+1}}-\frac{\sqrt{V_{t}}}{\alpha_{t}}$, the convergence will be guaranteed. Accordingly, &lt;a href=&#34;https://openreview.net/forum?id=ryQu7f-RZ&#34; target=&#34;_blank&#34;&gt;Reddi et al.&lt;/a&gt; proposed AMSGrad and AdamNC. However, these algorithms keep a non-decreasing $v_t$ or a long-memorization of gradients, which will slow down the training if a quite large gradient emerges. Therefore, a deep understanding on these non-convergence &lt;a href=&#34;#countereg&#34;&gt;counterexamples&lt;/a&gt; is needed.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;our-theoretical-analysis&#34;&gt;Our theoretical analysis&lt;/h3&gt;

&lt;p&gt;Some warm up conclusions:
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;../../img/AdaShift/warm-up1.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt;
  &lt;/div&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
  &lt;div class=&#34;column&#34;&gt;
    &lt;img src=&#34;../../img/AdaShift/warm-up2.png&#34; alt=&#34;drawing&#34; style=&#34;width:300px; hight:300px&#34;/&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Both $\beta_1$ and $\beta_2$  influence the direction and speed of optimization.&lt;/li&gt;
&lt;li&gt;Critical value of $C$, at which Adam gets into non-convergence, increases as $\beta_1$ and $\beta_2$  getting large.&lt;/li&gt;
&lt;li&gt;For any fixed $C$, as long as $\beta_1$ and $\beta_2$ large enough, non-convergence will disappear.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;The Cause of Non-Convergence: Unbalanced Step Size&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$v_t$ is positively correlated to the scale of gradient $g_t$&lt;/li&gt;
&lt;li&gt;It results in a decreasing step size for a large gradient and a increasing step size for a small gradient&lt;/li&gt;
&lt;li&gt;It is common for adaptive optimizers.&lt;/li&gt;
&lt;li&gt;Our tool: Net Update Factor - consider the effect of every gradients on the whole optimization process:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
net(g_t) \triangleq \sum_{i=t}^{\infty}\frac{~\alpha_i}{\sqrt{v_i}}[(1-\beta_1)\beta_1^{i-t}g_t] = k(g_t) \cdot g_t
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Our theoretical conclusions:

&lt;ol&gt;
&lt;li&gt;$k( C ) &amp;lt; k( -1 )$&lt;/li&gt;
&lt;li&gt;Decorrelating $v_t$ and $g_t$ will lead to convergence.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Rethinking the role of $v_t$:

&lt;ul&gt;
&lt;li&gt;$v_t$ reflects the gradient scale, and adjusts learning rate dynamically.&lt;/li&gt;
&lt;li&gt;In AdaShift, current $v_t$ is independent with $g_t$, but the distribution of $v_t$ is close to $g_t$’s, and changes dynamically with $g_t$’s.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;adashift-optimizer&#34;&gt;AdaShift Optimizer&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;../../img/AdaShift/adashift_algorithm.png&#34; alt=&#34;drawing&#34; style=&#34;width:800px; &#34;/&gt;&lt;/p&gt;

&lt;p&gt;The decorrelation operations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Temporal Shift: we postpone the calculation of $v_t$, using $g_{t-n}$ rather than $g_t$ to update $v_t$.

&lt;ul&gt;
&lt;li&gt;This decorrelation is based on the assumption of independence between mini-batches.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Spatial Decorrelation: Layer-wise adaptive learning rate.

&lt;ul&gt;
&lt;li&gt;For every seperate layer, we add a function $\phi$ over its gradient matrix ( or vector ), outputing a scalar to calculate shared $v_t$ for all parameters in this layer.&lt;/li&gt;
&lt;li&gt;In our experiments, we choose $\phi$ as maximum function over gradient matries. Better design of $\phi$ is left as future work.&lt;/li&gt;
&lt;li&gt;We no longer interpret $v_t$ as the second moment of $g_t$. Instead, it is a random variable, independent of $g_t$, while at the same time, reflects the overall gradient scale.&lt;/li&gt;
&lt;li&gt;Adam sometimes does not generalize better than SGD, which might be related with the excessive learning rate adaptation in Adam. As a compromise between SGD and Adam, AdaShift has an improvement in term of generalization performance.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A more intuitive explanation of AdaShift:
&lt;img src=&#34;../../img/AdaShift/IntuitiveExplain.jpg&#34; alt=&#34;drawing&#34; style=&#34;width:600px;/&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods</title>
      <link>https://qingruzhang.github.io/publication/adashift/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://qingruzhang.github.io/publication/adashift/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
