<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Qingru Zhang">

  
  
  
    
  
  <meta name="description" content="Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings.">

  
  <link rel="alternate" hreflang="en-us" href="https://qingruzhang.github.io/project/svrg_project/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.1726eea7ae2a52aa3678aa2e6020c6b1.css">

  

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://qingruzhang.github.io/project/svrg_project/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Qingru Zhang">
  <meta property="og:url" content="https://qingruzhang.github.io/project/svrg_project/">
  <meta property="og:title" content="A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed | Qingru Zhang">
  <meta property="og:description" content="Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings."><meta property="og:image" content="https://qingruzhang.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://qingruzhang.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-25T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-25T00:00:00&#43;00:00">
  

  


    











<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://qingruzhang.github.io/project/svrg_project/"
  },
  "headline": "A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed",
  
  "datePublished": "2019-09-25T00:00:00Z",
  "dateModified": "2019-09-25T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "**Qingru Zhang**"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Qingru Zhang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://qingruzhang.github.io/img/icon-512.png"
    }
  },
  "description": "Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings."
}
</script>

  

  


  


  





  <title>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed | Qingru Zhang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Qingru Zhang</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Internships</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments"><span>Honors & Awards</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article article-project">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</h1>

  
  <p class="page-subtitle">SVRG Non-Asymptotic Analysis</p>
  

  
    



<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/qingru-zhang/"><strong>Qingru Zhang</strong></a></span>, <span><a href="/authors/yuhuai-wu/">Yuhuai Wu</a></span>, <span><a href="/authors/fartash-faghri/">Fartash Faghri</a></span>, <span><a href="/authors/tianzong-zhang/">Tianzong Zhang</a></span>, <span><a href="/authors/jimmy-ba/">Jimmy Ba</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Sep 25, 2019
  </span>
  

  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/research-project/">Research Project</a></span>
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://qingruzhang.github.io/project/svrg_project/&amp;text=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://qingruzhang.github.io/project/svrg_project/&amp;t=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed&amp;body=https://qingruzhang.github.io/project/svrg_project/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://qingruzhang.github.io/project/svrg_project/&amp;title=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed%20https://qingruzhang.github.io/project/svrg_project/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://qingruzhang.github.io/project/svrg_project/&amp;title=A%20Non-asymptotic%20comparison%20of%20SVRG%20and%20SGD:%20tradeoffs%20between%20compute%20and%20speed" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    













<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary my-1 mr-1" href="/files/SVRG_Paper/Non-Asymptotic_Analysis_Paper.pdf" target="_blank" rel="noopener">
  PDF
</a>











  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="/files/SVRG_Paper/SlidesPre.pdf" target="_blank" rel="noopener">
  Slides
</a>







</div>


  
</div>



  <div class="article-container">

    <div class="article-style">
      

<hr />

<h3 id="background">Background</h3>

<p>SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence. Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence. Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs.</p>

<h3 id="our-contribution">Our Contribution</h3>

<ul>
<li>We show the exact expected loss of SVRG and SGD as a function of iterations and computational cost.</li>
<li>We discuss the trade-offs between the total computational cost and convergence performance.</li>
<li>We consider two different training regimes with and without label noise.

<ul>
<li>Under noisy labels, the analysis suggests SGD only outperforms SVRG under a mild total computational cost.</li>
<li>However, SGD always exhibits a faster convergence compared to SVRG when there is no label noise.</li>
</ul></li>
<li>Numerical experiments validate our theoretical predictions on both MNIST and CIFAR-10.

<ul>
<li>In particular, the comparison on underparameterized neural networks closely matches with our noisy least squares model prediction.</li>
<li>Whereas, the effect of overparameterization is captured by the regression model without label noise.</li>
</ul></li>
</ul>

<h3 id="svrg-inner-outer-loop-algorithm">SVRG:  inner-outer loop algorithm.</h3>

<p><strong>In the outer loop:</strong></p>

<ul>
<li>For every $T$ steps, we evaluate a large batch gradient $\overline{\mathbf{g}}=\frac{1}{N} \sum_{i}^{N} \nabla_{\boldsymbol{\theta}^{( m T )}} L_{i}$. where $N \gg b$, and $m$ is the outer loop index.</li>
<li>We store the parameters at reference points $\boldsymbol{\theta}^{(m T)}$.</li>
</ul>

<p><strong>In the inner loop:</strong>
$$\boldsymbol{\theta}^{(m T+t+1)}=\boldsymbol{\theta}^{(m T+t)}-\alpha^{(t)}\left(\hat{\boldsymbol{g}}^{(m T+t)}-\tilde{\boldsymbol{g}}^{(m T+t)}+\overline{\mathbf{g}}\right)$$</p>

<p>where $ \hat{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T+t)}} L_{i} $ is the current batch gradient and $ \tilde{\boldsymbol{g}}^{(m T+t)}=\frac{1}{b} \sum_{i}^{b} \nabla_{\boldsymbol{\theta}^{(m T)}} L_{i}$ is the old gradient.</p>

<h3 id="our-model-the-noisy-least-squares-regression-model">Our Model: The noisy least squares regression model</h3>

<p><a name="model"></a>
The input data is d-dimensional, and the output label is generated by a
linear teacher model with additive noise:</p>

<p>$$
\left(\boldsymbol{x}_{i}, \epsilon_{i}\right) \sim P_{x} \times P_{\epsilon} ; \quad y_{i}=\boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{*}+\epsilon_{i}
$$
where $\mathbb{E}\left[\boldsymbol{x}_{i}\right]=\boldsymbol{\mu} \in \mathbb{R}^{d} \text { and } \operatorname{Cov}\left(\boldsymbol{x}_{i}\right)=\Sigma, \mathbb{E}\left[\epsilon_{i}\right]=0, \operatorname{Var}\left(\epsilon_{i}\right)=\sigma_{y}^{2}$.</p>

<p><a name="assumptions"></a></p>

<h4 id="assumptions">Assumptions:</h4>

<ul>
<li>$\mu=\mathbf{0}$.</li>
<li>$\Sigma$ is diagonal.</li>
<li>$\boldsymbol{\theta}^{*}=\mathbf{0}$.</li>
</ul>

<h4 id="loss-function-and-bias-variance-decomposition">Loss Function and Bias-Variance Decomposition:</h4>

<p>Under our <a href="#assumptions">assumptions</a>, the <strong>expected loss</strong> can be simplified as a function of the second moment of the iterate.</p>

<p>$$
\begin{aligned}
L\left(\boldsymbol{\theta^{(t)}}\right)=&amp; \frac{1}{2} \mathbb{E}\left[\left(\boldsymbol{x_{i}}^{\top} \boldsymbol{\theta^{(t)}}-\epsilon_{i}\right)^{2}\right]\\<br />
=&amp; \frac{1}{2}\left(\operatorname{tr}\left(\Sigma \mathbb{E}\left[\boldsymbol{\theta^{(t)}} \boldsymbol{ \theta^{(t)}}\right]\right)+\sigma_{y}^{2}\right)\\<br />
=&amp; \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}
\end{aligned}
$$</p>

<h4 id="mini-batch-gradients">Mini-batch gradients:</h4>

<p>$$
\hat{\boldsymbol{g}}^{(t)}=\frac{1}{b} \sum_{i}^{b}\left(\boldsymbol{x}_{i} \boldsymbol{x}_{i}^{\top} \boldsymbol{\theta}^{(t)}-\boldsymbol{x}_{i} \epsilon_{i}\right)=X_{b} X_{b}^{\top} \boldsymbol{\theta}^{(t)}-\frac{1}{\sqrt{b}} X_{b} \boldsymbol{\epsilon}_{b}
$$</p>

<p>where $X_{b}=\frac{1}{\sqrt{b}}\left[\boldsymbol{x}_{1} ; \boldsymbol{x}_{2} ; \cdots ; \boldsymbol{x}_{b}\right] \in \mathbb{R}^{d \times b}$ and the target noise vector $\boldsymbol{\epsilon}_{b}=\left[\epsilon_{1} ; \epsilon_{2} ; \cdots ; \epsilon_{b}\right]^{\top} \in \mathbb{R}^{b}$.</p>

<h3 id="notations-and-definitions">Notations and Definitions</h3>

<p>$$
\begin{array}{l}{\mathrm{M}(\theta)=\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right], \quad \mathbf{m}(\theta)=\operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right]\right)} \\<br />
{\mathrm{C}(\boldsymbol{\theta})=\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top} \boldsymbol{\theta} \boldsymbol{\theta}^{\top} \boldsymbol{x} \boldsymbol{x}^{\top}\right]-\Sigma \mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{\top}\right] \Sigma} \\<br />
{V=\alpha^{2} \sigma_{y}^{2} \operatorname{diag}(\Sigma) \\<br />
R=(\mathrm{I}-\alpha \Sigma)^{2}+\frac{\alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)} \\<br />
{Q=\frac{2 \alpha^{2}}{b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right), \quad P=\mathrm{I}-\alpha \Sigma} \\<br />
{F=\frac{2 \alpha^{2}(N+b)}{N b}\left(\Sigma^{2}+\operatorname{diag}(\Sigma) \operatorname{diag}(\Sigma)^{\top}\right)}\end{array}
$$</p>

<h3 id="the-dynamic-of-sgd">The Dynamic of SGD</h3>

<p>$$
\mathrm{M}\Big(\boldsymbol{\theta}^{(t+1)}\Big)=\underbrace{(\mathrm{I}-\alpha \Sigma) \mathrm{M}\Big(\boldsymbol{\theta}^{(t)}\Big)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}
+ \underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big( \mathrm{M}(\boldsymbol{\theta}^{(t)})\Big)}_{  {2}:\text { input noise } }
+\underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{b} \Sigma}_{{3}: \text { label noise }}
$$</p>

<ul>
<li>The term $1$ leads to an exponential shrinkage of the loss due to the gradient descent update.</li>
<li>Since we are using a noisy gradient, the second term $2$ represents the variance of stochastic gradient caused by the random input $ X_b $.</li>
<li>The term $3$ comes from the label noise $ \epsilon_b $.</li>
</ul>

<h4 id="the-expected-second-moment-for-sgd">The expected second moment for SGD</h4>

<p>Given the <a href="#model">model</a> and our <a href="#assumptions">assumptions</a>, we have the second moment of parameter as:</p>

<p>$$
\mathbf{m}\Big(\boldsymbol{\theta}^{(t)}\Big)=R^{t}\left(\mathbf{m}\left(\boldsymbol{\theta}^{(0)}\right)-\frac{(\mathrm{I}-R)^{-1} V}{b}\right)+\frac{(\mathrm{I}-R)^{-1} V}{b}
$$</p>

<p>Since $L\left(\boldsymbol{\theta^{(t)}}\right)= \frac{1}{2} \operatorname{diag}(\Sigma)^{\top} \operatorname{diag}\left(\mathbb{E}\left[\boldsymbol{\theta}^{(t)} \boldsymbol{\theta}^{(t)^{\top}}\right]\right)+\frac{1}{2} \sigma_{y}^{2}$, we can draw the exact expression of the expected loss.</p>

<h3 id="the-dilemma-for-svrg">The Dilemma for SVRG</h3>

<h4 id="svrg-dynamic-under-our-model">SVRG Dynamic under Our Model</h4>

<p>$$
\begin{aligned} \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t+1)}\right)=&amp;\underbrace{(I-\alpha \Sigma) \mathrm{M}\left(\boldsymbol{\theta}^{(m T+t)}\right)(\mathrm{I}-\alpha \Sigma)}_{{1}: \text { gradient descent shrinkage }}+\underbrace{\frac{\alpha^{2}}{b} \mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T+t)})\Big)}_{ 2: \text { input noise } } \\<br />
&amp; + \underbrace{\frac{\alpha^{2} \sigma_{y}^{2}}{N} \Sigma}_{ 3: \text { label noise }} + \underbrace{\alpha^{2} \frac{N+b}{N b} \mathrm{C}\left(\mathrm{M}\left(\boldsymbol{\theta}^{(m T)}\right)\right)}_{ 4: \text {Gvariance due to } \tilde{\mathbf{g}}^{(m T+t)}}\\<br />
&amp; \underbrace{-\frac{\alpha^{2}}{b}\left(\mathrm{C}\Big(\mathrm{M}(\boldsymbol{\theta}^{(m T)}) P^{t}\Big) +\mathrm{C}\Big(P^{t} \mathrm{M}(\boldsymbol{\theta}^{(m T)})\Big)\right).}_{\mathbb{5} \text { Variance reduction from control variate }} \end{aligned}
$$</p>

<ul>
<li>First notice that terms ${1}, {2}, {3}$ reappear, contributed by the SGD update.</li>
<li>The additional terms, ${4}$ and ${5}$, are due to the control variate.</li>
<li>Observe that the variance reduction term ${5}$ decays exponentially throughout the inner loop, with decay rate $I-\alpha\Sigma$, which is the same term that governs the decay rate of the term ${1}$, hence resulting in a conflict between the two.</li>
<li>If we want to reduce the term ${1}$ as fast as possible, we would prefer a large learning rate, i.e. $\alpha \to \frac{1}{\lambda_{\max}(\Sigma)}$. But this will also make the boosts provided by the control variate diminish rapidly, leading to a poor variance reduction.</li>
<li>The term ${4}$ makes things even worse as it will maintain as a constant throughout the inner loop, contributing to an extra variance on top of the variance from standard SGD.</li>
<li>On the other hand, if one chooses a small learning rate for the variance reduction to take effect, this inevitably will make the decay rate for term ${1}$ smaller, resulting in a slower convergence.</li>
<li>A good news for SVRG is that the label noise (term ${3}$) is scaled by $\frac{b}{N}$, which lets SVRG converge to a lower loss value than SGD &ndash; a strict advantage of SVRG compared to SGD.</li>
</ul>

<h4 id="the-expected-second-moment-for-svrg">The expected second moment for SVRG</h4>

<p>Given the <a href="#model">model</a> and our <a href="#assumptions">assumptions</a>, we have the second moment of parameter as:</p>

<p>$$
\mathbf{m}\left(\boldsymbol{\theta}^{((m+1) T)}\right)=\lambda(\alpha, b, T, N, \Sigma) \mathbf{m}\left(\boldsymbol{\theta}^{(m T)}\right)+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} \frac{V}{N}
$$</p>

<p>where</p>

<p>$$
\lambda(\alpha, b, T, N, \Sigma)=R^{T}-\left(\sum_{k=0}^{T-1} R^{k} Q P^{-k}\right) P^{T-1}+\left(\mathrm{I}-R^{T}\right)(\mathrm{I}-R)^{-1} F
$$</p>

<h3 id="the-numerical-experiments-based-on-our-dynamic">The Numerical Experiments Based on Our Dynamic</h3>

<p>In all of our experiments, we compare SVRG and SGD under a fixed computation budget. For every budget, we run 10 leanring rates expentionally varying from 0.1 to 0.001, typically, for SGD. For SVRG, besides turning learning rate, we also choose different pairs of batch size and snapshot interval under a fixed budget. Then, at each step, we plot the minimum loss over these setting. In short, every line in the following plots need to run 10 or more setting for SGD; and 30 or more for SVRG. And the x-axis &ldquo;epoch&rdquo; denotes the total computational cost normalized by dataset.</p>

<div class="row">
  <div class="column">
    <!-- <img src="../../img/AdaShift/warm-up1.png" alt="drawing" style="width:300px; hight:300px"/> -->
    <div style="width: 350px; font-size:80%; text-align:center;"><img src="../../img/SVRG_Project/sim1.png" alt="alternate text 1" width="width" height="height" style="padding-bottom:0.5em;" /> With Label Noise </div>
  </div>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <div class="column">
    <div style="width:350px; font-size:80%; text-align:center;"><img src="../../img/SVRG_Project/sim2.png" alt="alternate text2" width="width" height="height" style="padding-bottom:0.5em;" />Without Label Noise</div>
    <!-- <img src="../../img/AdaShift/warm-up2.png" alt="drawing" style="width:300px; hight:300px"/> -->
  </div>
</div>

<p><strong>Our Conclusions:</strong></p>

<ul>
<li><strong>The case with label noise</strong>: The plot demonstrated an explicit trade-off between computational cost and convergence speed.

<ul>
<li>a crossing point of between SGD and SVRG appear, indicating SGD achieved a faster convergence speed in the first phase of the training, but converged to a higher loss, for all per-iteration compute cost.</li>
<li>The per-iteration computational cost does not seem to affect the time crossing point takes place. For all these three costs, the crossing points in the plot are at around the same time: $5.5$ epochs.</li>
</ul></li>
<li><strong>The case of no label noise</strong>: Both methods achieved linear convergence, while SGD achieved a much faster rate than SVRG, showing absolute dominance in this regime.</li>
</ul>

<h3 id="experiments-on-benchmark-datasets">Experiments on Benchmark Datasets</h3>

<h4 id="underparameterized-setting">Underparameterized Setting</h4>

<p>The results in underparametrized regime corresponds to the analysis with label noise. $ \mathbb{E}<em>{ \boldsymbol{x</em>{i}} \in P<em>{\boldsymbol{x}}  } \left[ \left(\boldsymbol{x</em>{i}}^{\top} \boldsymbol{\theta^{(t)}} - \epsilon_{i} \right)^{2} \right] = \mathbb{E}<em>{ \ boldsymbol{X} \in P</em>{ \boldsymbol{X} } } \left[ \frac{1}{N} \sum<em>{ \boldsymbol{x</em>{i}} \in \boldsymbol{X} } \left(\boldsymbol{x_{i}}^{\top} \boldsymbol{\theta^{(t)}} - \epsilon_{i} \right)^{2} \right]  $</p>

<p><strong>Logistic Regression on MNIST</strong></p>

<p><img src="../../img/SVRG_Project/logreg.png" alt="drawing" style="width:450px; "/></p>

<p><strong>MLP-10-10 on MNIST</strong>
<img src="../../img/SVRG_Project/mlp10.png" alt="drawing" style="width:450px; "/></p>

<p><strong>Underparametrized CNN on CIFAR-10</strong>
<img src="../../img/SVRG_Project/sscnn.png" alt="drawing" style="width:450px; "/></p>

<h4 id="overparametrized-setting">Overparametrized Setting</h4>

<p>The results in overparametrized regime corresponds to the analysis without label noise.</p>

<p><strong>Overparametrized MLP on MNIST</strong>
<img src="../../img/SVRG_Project/mlp1024.png" alt="drawing" style="width:450px; "/></p>

<p><strong>Overparametrized CNN on CIFAR-10</strong>
<img src="../../img/SVRG_Project/cnn.png" alt="drawing" style="width:450px; "/></p>

    </div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/optimization/">Optimization</a>
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/svrg/">SVRG</a>
  
  <a class="badge badge-light" href="/tags/non-asymptotic/">Non-asymptotic</a>
  
  <a class="badge badge-light" href="/tags/variance-reduction/">Variance reduction</a>
  
</div>


    








  
  
    
  
  






  
  
  
  
  <div class="media author-card">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/qingru-zhang/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>




    


    
    

    
    
    

    
    
    
      <h2>Publications</h2>
      
        
          








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/svrg_nonasymptotic_analysis/" >A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed</a>
    </h3>

    
    <div class="article-style">
      Stochastic gradient descent (SGD), which  trades off noisy gradient updates for computational efficiency, is the de-facto optimization â€¦
    </div>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        



  
  <span><a href="/authors/qingru-zhang/"><strong>Qingru Zhang</strong></a></span>, <span><a href="/authors/yuhuai-wu/">Yuhuai Wu</a></span>, <span><a href="/authors/fartash-faghri/">Fartash Faghri</a></span>, <span><a href="/authors/tianzong-zhang/">Tianzong Zhang</a></span>, <span><a href="/authors/jimmy-ba/">Jimmy Ba</a></span>

      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/SVRG_Paper/Non-Asymptotic_Analysis_Paper.pdf" target="_blank" rel="noopener">
  PDF
</a>







  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/svrg_project/">
    Project
  </a>
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/files/SVRG_Paper/SlidesPre.pdf" target="_blank" rel="noopener">
  Slides
</a>







    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

        
      
    

    
    
    

  </div>
</article>



      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
