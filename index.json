[{"authors":["admin"],"categories":null,"content":"I am a senior undergraduate student from Shanghai Jiao Tong University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://qingruzhang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a senior undergraduate student from Shanghai Jiao Tong University.","tags":null,"title":"Qingru Zhang","type":"authors"},{"authors":["Zhiming Zhou*","**Qingru Zhang***(equal contribution)","Guansong Lu","Hongwei Wang","Weinan Zhang","Yong Yu"],"categories":null,"content":"Further details on your publication can be written here using Markdown for formatting. This text will be displayed on the Publication Detail page.\n","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"2b1822f84c5861e87b534b05e7053230","permalink":"https://qingruzhang.github.io/publication/adashift/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/adashift/","section":"publication","summary":"Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient g_t and the second-moment term v_t in Adam (t is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such biased step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating v_t and g_t will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates v_t and g_t by temporal shifting, i.e., using temporally shifted gradient g_{t-n} to calculate v_t. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization.","tags":null,"title":"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods","type":"publication"},{"authors":["First author's name","Second author's name"],"categories":null,"content":"Further details on your publication can be written here using Markdown for formatting. This text will be displayed on the Publication Detail page.\n","date":1480896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480896000,"objectID":"12cbee88b745a830ec715538a531c340","permalink":"https://qingruzhang.github.io/publication/exp/","publishdate":"2016-12-05T00:00:00Z","relpermalink":"/publication/exp/","section":"publication","summary":"The abstract. Markdown and math can be used (note that math may require escaping as detailed in the red alert box below).","tags":null,"title":"A publication title, such as title of a paper","type":"publication"}]