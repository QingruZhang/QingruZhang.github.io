[{"authors":["admin"],"categories":null,"content":"I am a senior undergraduate student in Shanghai Jiao Tong University, majoring in Cyber Science and Engineer. I am also a member of Zhiyuan Honors Program.\nPreviously, I am fortunate to work as a research assistant in Apex Data and Knowledge Mangement Lab, advised by Prof. Weinan Zhang and Prof. Yong Yu. From Jul. 2019 to Oct. 2019, I worked as a visiting research intern at Vector Institute and Machine Leraning Group of UofT, advised by Prof.Jimmy Ba.\nI am planning to apply for Ph.D. programs that start in Fall 2020. My research interests lie in optimization and machine learning, especially first-order optimization, gradient estimization and applications of optimization in deep learning. I am also interested in other fundamental questions, like generalization of neural networks, and open to exploring other interesting topics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://qingruzhang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a senior undergraduate student in Shanghai Jiao Tong University, majoring in Cyber Science and Engineer. I am also a member of Zhiyuan Honors Program.\nPreviously, I am fortunate to work as a research assistant in Apex Data and Knowledge Mangement Lab, advised by Prof. Weinan Zhang and Prof. Yong Yu. From Jul. 2019 to Oct. 2019, I worked as a visiting research intern at Vector Institute and Machine Leraning Group of UofT, advised by Prof.","tags":null,"title":"Qingru Zhang","type":"authors"},{"authors":["**Qingru Zhang**","Yuhuai Wu","Fartash Faghri","Tianzong Zhang","Jimmy Ba"],"categories":["Research Project"],"content":" Background SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence. Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence. Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs.\nOur Contribution  We show the exact expected loss of SVRG and SGD as a function of iterations and computational cost. We discuss the trade-offs between the total computational cost and convergence performance. We consider two different training regimes with and without label noise.  Under noisy labels, the analysis suggests SGD only outperforms SVRG under a mild total computational cost. However, SGD always exhibits a faster convergence compared to SVRG when there is no label noise.  Numerical experiments validate our theoretical predictions on both MNIST and CIFAR-10.  In particular, the comparison on underparameterized neural networks closely matches with our noisy least squares model prediction. Whereas, the effect of overparameterization is captured by the regression model without label noise.   SVRG: inner-outer loop algorithm. In the outer loop:\n For every $T$ steps, we evaluate a large batch gradient $\\overline{\\mathbf{g}}=\\frac{1}{N} \\sum_{i}^{N} \\nabla_{\\boldsymbol{\\theta}^{( m T )}} L_{i}$. where $N \\gg b$, and $m$ is the outer loop index. We store the parameters at reference points $\\boldsymbol{\\theta}^{(m T)}$.  In the inner loop: $$\\boldsymbol{\\theta}^{(m T+t+1)}=\\boldsymbol{\\theta}^{(m T+t)}-\\alpha^{(t)}\\left(\\hat{\\boldsymbol{g}}^{(m T+t)}-\\tilde{\\boldsymbol{g}}^{(m T+t)}+\\overline{\\mathbf{g}}\\right)$$\nwhere $ \\hat{\\boldsymbol{g}}^{(m T+t)}=\\frac{1}{b} \\sum_{i}^{b} \\nabla_{\\boldsymbol{\\theta}^{(m T+t)}} L_{i} $ is the current batch gradient and $ \\tilde{\\boldsymbol{g}}^{(m T+t)}=\\frac{1}{b} \\sum_{i}^{b} \\nabla_{\\boldsymbol{\\theta}^{(m T)}} L_{i}$ is the old gradient.\nOur Model: The noisy least squares regression model  The input data is d-dimensional, and the output label is generated by a linear teacher model with additive noise:\n$$ \\left(\\boldsymbol{x}_{i}, \\epsilon_{i}\\right) \\sim P_{x} \\times P_{\\epsilon} ; \\quad y_{i}=\\boldsymbol{x}_{i}^{\\top} \\boldsymbol{\\theta}^{*}+\\epsilon_{i} $$ where $\\mathbb{E}\\left[\\boldsymbol{x}_{i}\\right]=\\boldsymbol{\\mu} \\in \\mathbb{R}^{d} \\text { and } \\operatorname{Cov}\\left(\\boldsymbol{x}_{i}\\right)=\\Sigma, \\mathbb{E}\\left[\\epsilon_{i}\\right]=0, \\operatorname{Var}\\left(\\epsilon_{i}\\right)=\\sigma_{y}^{2}$.\n\nAssumptions:  $\\mu=\\mathbf{0}$. $\\Sigma$ is diagonal. $\\boldsymbol{\\theta}^{*}=\\mathbf{0}$.  Loss Function and Bias-Variance Decomposition: Under our assumptions, the expected loss can be simplified as a function of the second moment of the iterate.\n$$ \\begin{aligned} L\\left(\\boldsymbol{\\theta^{(t)}}\\right)=\u0026amp; \\frac{1}{2} \\mathbb{E}\\left[\\left(\\boldsymbol{x_{i}}^{\\top} \\boldsymbol{\\theta^{(t)}}-\\epsilon_{i}\\right)^{2}\\right]\\\\\n=\u0026amp; \\frac{1}{2}\\left(\\operatorname{tr}\\left(\\Sigma \\mathbb{E}\\left[\\boldsymbol{\\theta^{(t)}} \\boldsymbol{ \\theta^{(t)}}\\right]\\right)+\\sigma_{y}^{2}\\right)\\\\\n=\u0026amp; \\frac{1}{2} \\operatorname{diag}(\\Sigma)^{\\top} \\operatorname{diag}\\left(\\mathbb{E}\\left[\\boldsymbol{\\theta}^{(t)} \\boldsymbol{\\theta}^{(t)^{\\top}}\\right]\\right)+\\frac{1}{2} \\sigma_{y}^{2} \\end{aligned} $$\nMini-batch gradients: $$ \\hat{\\boldsymbol{g}}^{(t)}=\\frac{1}{b} \\sum_{i}^{b}\\left(\\boldsymbol{x}_{i} \\boldsymbol{x}_{i}^{\\top} \\boldsymbol{\\theta}^{(t)}-\\boldsymbol{x}_{i} \\epsilon_{i}\\right)=X_{b} X_{b}^{\\top} \\boldsymbol{\\theta}^{(t)}-\\frac{1}{\\sqrt{b}} X_{b} \\boldsymbol{\\epsilon}_{b} $$\nwhere $X_{b}=\\frac{1}{\\sqrt{b}}\\left[\\boldsymbol{x}_{1} ; \\boldsymbol{x}_{2} ; \\cdots ; \\boldsymbol{x}_{b}\\right] \\in \\mathbb{R}^{d \\times b}$ and the target noise vector $\\boldsymbol{\\epsilon}_{b}=\\left[\\epsilon_{1} ; \\epsilon_{2} ; \\cdots ; \\epsilon_{b}\\right]^{\\top} \\in \\mathbb{R}^{b}$.\nNotations and Definitions $$ \\begin{array}{l}{\\mathrm{M}(\\theta)=\\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{\\top}\\right], \\quad \\mathbf{m}(\\theta)=\\operatorname{diag}\\left(\\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{\\top}\\right]\\right)} \\\\\n{\\mathrm{C}(\\boldsymbol{\\theta})=\\mathbb{E}\\left[\\boldsymbol{x} \\boldsymbol{x}^{\\top} \\boldsymbol{\\theta} \\boldsymbol{\\theta}^{\\top} \\boldsymbol{x} \\boldsymbol{x}^{\\top}\\right]-\\Sigma \\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{\\top}\\right] \\Sigma} \\\\\n{V=\\alpha^{2} \\sigma_{y}^{2} \\operatorname{diag}(\\Sigma) \\\\\nR=(\\mathrm{I}-\\alpha \\Sigma)^{2}+\\frac{\\alpha^{2}}{b}\\left(\\Sigma^{2}+\\operatorname{diag}(\\Sigma) \\operatorname{diag}(\\Sigma)^{\\top}\\right)} \\\\\n{Q=\\frac{2 \\alpha^{2}}{b}\\left(\\Sigma^{2}+\\operatorname{diag}(\\Sigma) \\operatorname{diag}(\\Sigma)^{\\top}\\right), \\quad P=\\mathrm{I}-\\alpha \\Sigma} \\\\\n{F=\\frac{2 \\alpha^{2}(N+b)}{N b}\\left(\\Sigma^{2}+\\operatorname{diag}(\\Sigma) \\operatorname{diag}(\\Sigma)^{\\top}\\right)}\\end{array} $$\nThe Dynamic of SGD $$ \\mathrm{M}\\left(\\boldsymbol{\\theta}^{(t+1)}\\right)=\\underbrace{(\\mathrm{I}-\\alpha \\Sigma) \\mathrm{M}\\left(\\boldsymbol{\\theta}^{(t)}\\right)(\\mathrm{I}-\\alpha \\Sigma)}_{{1}: \\text { gradient descent shrinkage }} + \\underbrace{\\frac{\\alpha^{2}}{b} \\mathrm{C}\\Big( \\mathrm{M}(\\boldsymbol{\\theta}^{(t)})\\Big)}_{ {2}:\\text { input noise } } +\\underbrace{\\frac{\\alpha^{2} \\sigma_{y}^{2}}{b} \\Sigma}_{{3}: \\text { label noise }} $$\n The term $1$ leads to an exponential shrinkage of the loss due to the gradient descent update. Since we are using a noisy gradient, the second term $2$ represents the variance of stochastic gradient caused by the random input $ X_b $. The term $3$ comes from the label noise $ \\epsilon_b $.  The expected second moment for SGD Given the model and our assumptions, we have the second moment of parameter as:\n$$ \\mathbf{m}\\left(\\boldsymbol{\\theta}^{(t)}\\right)=R^{t}\\left(\\mathbf{m}\\left(\\boldsymbol{\\theta}^{(0)}\\right)-\\frac{(\\mathrm{I}-R)^{-1} V}{b}\\right)+\\frac{(\\mathrm{I}-R)^{-1} V}{b} $$\nSince $L\\left(\\boldsymbol{\\theta^{(t)}}\\right)= \\frac{1}{2} \\operatorname{diag}(\\Sigma)^{\\top} \\operatorname{diag}\\left(\\mathbb{E}\\left[\\boldsymbol{\\theta}^{(t)} \\boldsymbol{\\theta}^{(t)^{\\top}}\\right]\\right)+\\frac{1}{2} \\sigma_{y}^{2}$, we can draw the exact expression of the expected loss.\nThe Dilemma for SVRG SVRG Dynamic under Our Model $$ \\begin{aligned} \\mathrm{M}\\left(\\boldsymbol{\\theta}^{(m T+t+1)}\\right)=\u0026amp;\\underbrace{(I-\\alpha \\Sigma) \\mathrm{M}\\left(\\boldsymbol{\\theta}^{(m T+t)}\\right)(\\mathrm{I}-\\alpha \\Sigma)}_{{1}: \\text { gradient descent shrinkage }}+\\underbrace{\\frac{\\alpha^{2}}{b} \\mathrm{C}\\Big(\\mathrm{M}(\\boldsymbol{\\theta}^{(m T+t)})\\Big)}_{ 2: \\text { input noise } } \\\\\n\u0026amp; + \\underbrace{\\frac{\\alpha^{2} \\sigma_{y}^{2}}{N} \\Sigma}_{ 3: \\text { label noise }} + \\underbrace{\\alpha^{2} \\frac{N+b}{N b} \\mathrm{C}\\left(\\mathrm{M}\\left(\\boldsymbol{\\theta}^{(m T)}\\right)\\right)}_{ 4: \\text {Gvariance due to } \\tilde{\\mathbf{g}}^{(m T+t)}}\\\\\n\u0026amp; \\underbrace{-\\frac{\\alpha^{2}}{b}\\left(\\mathrm{C}\\Big(\\mathrm{M}(\\boldsymbol{\\theta}^{(m T)}) P^{t}\\Big) +\\mathrm{C}\\Big(P^{t} \\mathrm{M}(\\boldsymbol{\\theta}^{(m T)})\\Big)\\right).}_{\\mathbb{5} \\text { Variance reduction from control variate }} \\end{aligned} $$\n First notice that terms ${1}, {2}, {3}$ reappear, contributed by the SGD update. The additional terms, ${4}$ and ${5}$, are due to the control variate. Observe that the variance reduction term ${5}$ decays exponentially throughout the inner loop, with decay rate $I-\\alpha\\Sigma$, which is the same term that governs the decay rate of the term ${1}$, hence resulting in a conflict between the two. If we want to reduce the term ${1}$ as fast as possible, we would prefer a large learning rate, i.e. $\\alpha \\to \\frac{1}{\\lambda_{\\max}(\\Sigma)}$. But this will also make the boosts provided by the control variate diminish rapidly, leading to a poor variance reduction. The term ${4}$ makes things even worse as it will maintain as a constant throughout the inner loop, contributing to an extra variance on top of the variance from standard SGD. On the other hand, if one chooses a small learning rate for the variance reduction to take effect, this inevitably will make the decay rate for term ${1}$ smaller, resulting in a slower convergence. A good news for SVRG is that the label noise (term ${3}$) is scaled by $\\frac{b}{N}$, which lets SVRG converge to a lower loss value than SGD \u0026ndash; a strict advantage of SVRG compared to SGD.  The expected second moment for SVRG Given the model and our assumptions, we have the second moment of parameter as:\n$$ \\mathbf{m}\\left(\\boldsymbol{\\theta}^{((m+1) T)}\\right)=\\lambda(\\alpha, b, T, N, \\Sigma) \\mathbf{m}\\left(\\boldsymbol{\\theta}^{(m T)}\\right)+\\left(\\mathrm{I}-R^{T}\\right)(\\mathrm{I}-R)^{-1} \\frac{V}{N} $$\nwhere\n$$ \\lambda(\\alpha, b, T, N, \\Sigma)=R^{T}-\\left(\\sum_{k=0}^{T-1} R^{k} Q P^{-k}\\right) P^{T-1}+\\left(\\mathrm{I}-R^{T}\\right)(\\mathrm{I}-R)^{-1} F $$\nThe Numerical Experiments Based on Our Dynamic In all of our experiments, we compare SVRG and SGD under a fixed computation budget. For every budget, we run 10 leanring rates expentionally varying from 0.1 to 0.001, typically, for SGD. For SVRG, besides turning learning rate, we also choose different pairs of batch size and snapshot interval under a fixed budget. Then, at each step, we plot the minimum loss over these setting. In short, every line in the following plots need to run 10 or more setting for SGD; and 30 or more for SVRG. And the x-axis \u0026ldquo;epoch\u0026rdquo; denotes the total computational cost normalized by dataset.\n-- With Label Noise   \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; Without Label Noise --   Our Conclusions:\n The case with label noise: The plot demonstrated an explicit trade-off between computational cost and convergence speed.  a crossing point of between SGD and SVRG appear, indicating SGD achieved a faster convergence speed in the first phase of the training, but converged to a higher loss, for all per-iteration compute cost. The per-iteration computational cost does not seem to affect the time crossing point takes place. For all these three costs, the crossing points in the plot are at around the same time: $5.5$ epochs.  The case of no label noise: Both methods achieved linear convergence, while SGD achieved a much faster rate than SVRG, showing absolute dominance in this regime.  Experiments on Benchmark Datasets Underparameterized Setting The results in underparametrized regime corresponds to the analysis with label noise.\nLogistic Regression on MNIST\nMLP-10-10 on MNIST Underparametrized CNN on CIFAR-10 Overparametrized Setting The results in overparametrized regime corresponds to the analysis without label noise.\nOverparametrized MLP on MNIST Overparametrized CNN on CIFAR-10 ","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"214f34e9e78ab4665ec38c09eff079d1","permalink":"https://qingruzhang.github.io/project/svrg_project/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/project/svrg_project/","section":"project","summary":"Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings.","tags":["Optimization","Deep Learning","SVRG","Non-asymptotic","Variance reduction"],"title":"A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed","type":"project"},{"authors":["**Qingru Zhang**","Yuhuai Wu","Fartash Faghri","Tianzong Zhang","Jimmy Ba"],"categories":null,"content":"Further details.\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"c5e8ed5fa2435cb00ee5f47315576403","permalink":"https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/publication/svrg_nonasymptotic_analysis/","section":"publication","summary":"Stochastic gradient descent (SGD), which  trades off noisy gradient updates for computational efficiency, is the de-facto optimization algorithm to solve large-scale machine learning problems. SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence.   Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence.  Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs. In this paper, we present a non-asymptotic analysis of SVRG under a noisy least squares regression problem. Our primary focus is to compare the exact loss of SVRG to that of SGD at each iteration t. We show that the learning dynamics of our regression model closely matches with that of neural networks on MNIST and CIFAR-10 for both the underparameterized and the overparameterized models. Our analysis and experimental results suggest there is a trade-off between the computational cost and the convergence speed in underparametrized neural networks. SVRG outperforms SGD after a few epochs in this regime. However, SGD is shown to always outperform SVRG in the overparameterized regime.","tags":null,"title":"A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed","type":"publication"},{"authors":["**Qingru Zhang**"],"categories":["Research Project"],"content":" Conducted a survey on the generalization theory and its related work. Studied the proof of various generalization bounds and obtained a deep understanding on PAC-Bayesian framework, Rademacher complexity and VC-dimension. Considered to explain the generalization of neural network from dataset structure and Presented a group talk about generalization bounds and rethinking generalization.  ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"09083b8e698c78ea5741166316a2bf7e","permalink":"https://qingruzhang.github.io/project/generalization_survey_project/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/project/generalization_survey_project/","section":"project","summary":"I conducted a survey on various generalization bounds and the related work, and obtained a understanding on PAC-Bayesian framework, Rademacher complexity and VC-dimension.","tags":["Generalization","Machine Learning","PAC-Bayesian"],"title":"A Survey on the generalization theory of neural network","type":"project"},{"authors":["Zhiming Zhou*","**Qingru Zhang***","Guansong Lu","Hongwei Wang","Weinan Zhang","Yong Yu"],"categories":["Research Project"],"content":" The Nonconvergence of Adam Update rule of Adam: $$ \\begin{aligned} \u0026amp; g_t = \\nabla f_t\\\\\n\u0026amp; m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\\label{eq:1}\\\\\n\u0026amp; v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\\label{ew:2}\\\\\\\n\u0026amp;\\theta_{t+1} = \\theta_t - \\frac{\\alpha_t}{\\sqrt{v_t}}m_t \\end{aligned} $$\nReddi et al. pointed out two type of non-convergence problem for Adam: \n Sequential Counterexample:  $$f_{t}(\\theta)=\\left\\{\\begin{array}{ll}{C \\theta,} \u0026amp; {\\text { if } t \\bmod d=1} \\\\ {-\\theta,} \u0026amp; {\\text { otherwise }}\\end{array}\\right.$$\n Stochastic Counterexample:  $$ f_{t}(\\theta)=\\left\\{\\begin{array}{ll}{C \\theta,} \u0026amp; {\\text { with probability } p=\\frac{1+\\delta}{C+1}} \\\\ {-\\theta,} \u0026amp; {\\text { with probability } 1-p=\\frac{C-\\delta}{C+1}}\\end{array}\\right. $$\nand they claim after fixing the issue about positive definiteness of $\\Gamma_{t+1}=\\frac{\\sqrt{V_{t+1}}}{\\alpha_{t+1}}-\\frac{\\sqrt{V_{t}}}{\\alpha_{t}}$, the convergence will be guaranteed. Accordingly, Reddi et al. proposed AMSGrad and AdamNC. However, these algorithms keep a non-decreasing $v_t$ or a long-memorization of gradients, which will slow down the training if a quite large gradient emerges. Therefore, a deep understanding on these non-convergence counterexamples is needed.\nOur theoretical analysis Some warm up conclusions:  \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;  \n Both $\\beta_1$ and $\\beta_2$ influence the direction and speed of optimization. Critical value of $C$, at which Adam gets into non-convergence, increases as $\\beta_1$ and $\\beta_2$ getting large. For any fixed $C$, as long as $\\beta_1$ and $\\beta_2$ large enough, non-convergence will disappear.  The Cause of Non-Convergence: Unbalanced Step Size\n $v_t$ is positively correlated to the scale of gradient $g_t$ It results in a decreasing step size for a large gradient and a increasing step size for a small gradient It is common for adaptive optimizers. Our tool: Net Update Factor - consider the effect of every gradients on the whole optimization process:  $$ net(g_t) \\triangleq \\sum_{i=t}^{\\infty}\\frac{~\\alpha_i}{\\sqrt{v_i}}[(1-\\beta_1)\\beta_1^{i-t}g_t] = k(g_t) \\cdot g_t $$\n Our theoretical conclusions:  $k( C ) \u0026lt; k( -1 )$ Decorrelating $v_t$ and $g_t$ will lead to convergence.  Rethinking the role of $v_t$:  $v_t$ reflects the gradient scale, and adjusts learning rate dynamically. In AdaShift, current $v_t$ is independent with $g_t$, but the distribution of $v_t$ is close to $g_t$’s, and changes dynamically with $g_t$’s.   AdaShift Optimizer The decorrelation operations:\n Temporal Shift: we postpone the calculation of $v_t$, using $g_{t-n}$ rather than $g_t$ to update $v_t$.  This decorrelation is based on the assumption of independence between mini-batches.  Spatial Decorrelation: Layer-wise adaptive learning rate.  For every seperate layer, we add a function $\\phi$ over its gradient matrix ( or vector ), outputing a scalar to calculate shared $v_t$ for all parameters in this layer. In our experiments, we choose $\\phi$ as maximum function over gradient matries. Better design of $\\phi$ is left as future work. We no longer interpret $v_t$ as the second moment of $g_t$. Instead, it is a random variable, independent of $g_t$, while at the same time, reflects the overall gradient scale. Adam sometimes does not generalize better than SGD, which might be related with the excessive learning rate adaptation in Adam. As a compromise between SGD and Adam, AdaShift has an improvement in term of generalization performance.  A more intuitive explanation of AdaShift:   ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"01d582f250d037671e0d1c4e0e8a1786","permalink":"https://qingruzhang.github.io/project/adashiftproject/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/project/adashiftproject/","section":"project","summary":"We analysis and solve the non-convergence issue of Adam.","tags":["Optimization","Machine Learning","Adam"],"title":"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods","type":"project"},{"authors":["Zhiming Zhou*","**Qingru Zhang***","Guansong Lu","Hongwei Wang","Weinan Zhang","Yong Yu"],"categories":null,"content":"","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"2b1822f84c5861e87b534b05e7053230","permalink":"https://qingruzhang.github.io/publication/adashift/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/adashift/","section":"publication","summary":"Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient g_t and the second-moment term v_t in Adam (t is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such biased step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating v_t and g_t will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates v_t and g_t by temporal shifting, i.e., using temporally shifted gradient g_{t-n} to calculate v_t. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization.","tags":null,"title":"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods","type":"publication"}]