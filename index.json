[{"authors":["admin"],"categories":null,"content":"I am a senior undergraduate student from Shanghai Jiao Tong University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://qingruzhang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a senior undergraduate student from Shanghai Jiao Tong University.","tags":null,"title":"Qingru Zhang","type":"authors"},{"authors":["**Qingru Zhang**","Yuhuai Wu","Fartash Faghri","Tianzong Zhang","Jimmy Ba"],"categories":["Research Project"],"content":"Further details.\n","date":1569369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569369600,"objectID":"214f34e9e78ab4665ec38c09eff079d1","permalink":"https://qingruzhang.github.io/project/svrg_project/","publishdate":"2019-09-25T00:00:00Z","relpermalink":"/project/svrg_project/","section":"project","summary":"Non-asymptotic analysis of SGD and SVRG, showing the strength of each algorithm in convergence speed and computational cost, in both under-parametrized and over-parametrized settings.","tags":["Optimization","Deep Learning","SVRG","Non-asymptotic","Variance reduction"],"title":"A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed","type":"project"},{"authors":["**Qingru Zhang**","Yuhuai Wu","Fartash Faghri","Tianzong Zhang","Jimmy Ba"],"categories":null,"content":"Further details.\n","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"c5e8ed5fa2435cb00ee5f47315576403","permalink":"https://qingruzhang.github.io/publication/svrg_nonasymptotic_analysis/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/publication/svrg_nonasymptotic_analysis/","section":"publication","summary":"Stochastic gradient descent (SGD), which  trades off noisy gradient updates for computational efficiency, is the de-facto optimization algorithm to solve large-scale machine learning problems. SGD can make rapid learning progress by performing updates using subsampled training data, but the noisy updates also lead to slow asymptotic convergence.   Several variance reduction algorithms, such as SVRG, introduce control variates to obtain a lower variance gradient estimate and faster convergence.  Despite their appealing asymptotic guarantees, SVRG-like algorithms have not been widely adopted in deep learning. The traditional asymptotic analysis in stochastic optimization provides limited insight into training deep learning models under a fixed number of epochs. In this paper, we present a non-asymptotic analysis of SVRG under a noisy least squares regression problem. Our primary focus is to compare the exact loss of SVRG to that of SGD at each iteration t. We show that the learning dynamics of our regression model closely matches with that of neural networks on MNIST and CIFAR-10 for both the underparameterized and the overparameterized models. Our analysis and experimental results suggest there is a trade-off between the computational cost and the convergence speed in underparametrized neural networks. SVRG outperforms SGD after a few epochs in this regime. However, SGD is shown to always outperform SVRG in the overparameterized regime.","tags":null,"title":"A Non-asymptotic comparison of SVRG and SGD: tradeoffs between compute and speed","type":"publication"},{"authors":["Zhiming Zhou*","**Qingru Zhang***","Guansong Lu","Hongwei Wang","Weinan Zhang","Yong Yu"],"categories":["Research Project"],"content":"Further details. ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"01d582f250d037671e0d1c4e0e8a1786","permalink":"https://qingruzhang.github.io/project/adashiftproject/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/project/adashiftproject/","section":"project","summary":"We analysis and solve the non-convergence issue of Adam.","tags":["Optimization","Machine Learning","Adam"],"title":"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods","type":"project"},{"authors":["Zhiming Zhou*","**Qingru Zhang***","Guansong Lu","Hongwei Wang","Weinan Zhang","Yong Yu"],"categories":null,"content":"","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"2b1822f84c5861e87b534b05e7053230","permalink":"https://qingruzhang.github.io/publication/adashift/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/adashift/","section":"publication","summary":"Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient g_t and the second-moment term v_t in Adam (t is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such biased step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating v_t and g_t will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates v_t and g_t by temporal shifting, i.e., using temporally shifted gradient g_{t-n} to calculate v_t. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization.","tags":null,"title":"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods","type":"publication"}]